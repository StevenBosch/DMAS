\section{Discussion}
In this paper a multi-agent system approach was taken to identify the effects of experience on learning during a group task, that is, riot control. Several simulations were run using different learning rates - this way simulating the amount of experience of the agents - and different biases towards an action. Results showed some significant differences in performance, though the effects were marginal. Different biases did show some clear differences in performance, showing an inverse relationship between speed and success given the two actions. Moreover, a preference for actions do indeed affect performance in both speed and success. 

The fact that learning (and in extension different learning rates) had no noticable effect on success, suggests that the group performance of cops in mitigating civil wars does not depend on personal experience to a large degree. This can be explained by a number of factors: (1) the amount of agents, (2) the uniformity of the map, (3) the learning mechanism and (4) the rewards. (1) Because of the large number of cops (4000) in the simulation, high success and low success of the different cop behaviors cancel each other out in the total success of the simulation. Second, because the map is highly uniform in both its structure (symmetric, identical grids) and population, behavior is not controlled by anything other than their utility tables and randomness (the populations in the cells were taken from normal distributions). Even though we defined situations the cops can get in (cops majority, civilians majority etc.), the agents that occur together in these situations are different every time. In other words, everything is spread out too much. (3) Because the cops learn based on the group success per cell, oppossing behaviors in one cell can get the same reinforcement, which globally leads to random behavior. This is probably why figure \ref{fig:utilities} shows that most of the averaged utilities converge somewhere around 0.5. (4) Finally, because the rewards for both saving civilians and incapacitating hostiles are the same, even if the group learned as a whole (as is shown in figure \ref{fig:utShoot} and \ref{fig:utSave}), the global success was still hardly affected. What is curious though, is why the cops then learned this behavior if not for success. This is a question we have yet to research.
All in all, the combination of these factors caused the effect of individual learning to be negligible for group performance.

The initialized biases, however, did result in big changes in performances. Whereas a bias for saving greatly sped up the mitigation at the cost of success, incapacitating hostiles greatly increased success at the cost of speed. This can be explained by the fact that the stop criterion in the simulation was set at no more civilians left in the simulation. If cops were heavily biased towards saving them, civilians would both be saved more quickly and be killed more quickly, because few hostiles would get incapacitated. This would result in a quick mitigation, but a relatively low success. On the other hand if cops were heavily biased towards incapacitating, hostiles would be removed from the simulation quickly, resulting in less civilians and cops being killed. This would result in a high success rate but an overall low speed, since it takes the cops longer to save all the civilians.

These results indicate that, instead of the composition of the team with regard to experience, the focus should lie on strategy selection, based on the goal at hand. So following these results it would be ore important for a police force to determine a strategy beforehand and let their cops go into the field using that strategy than to look very carefully at the composition of the team in terms of experience. Of course these results need refinement, because the simulation was an extremely simplified approximation of reality. With hostiles only able to hurt, and the inability for hostiles and civilians to walk through the city, the simulation can clearly be considered a low-fidelity simulation. Moreover, the cops only have one decision to make and will only walk when all hostiles are gone in his area.

Considering these downfalls and the potential of such simulations, we recommend a number of improvements and extensions for future research. Important improvements would consist of making the agents more intelligent and the matrix more city like to provide more accurate results. This research only focussed on the comparison between saving and incapacitating. By implementing more realistic behavior, it would be possible to 
compare the results of different strategies or combinations thereof. Moreover, new strategies may be designed and tested by use of a multi-agent simulation. Showing what strategies show the most 
potential and how individual cops affect these strategies could ultimately result in better mitigation of civil wars and better training programs. 

\section{Conclusion}
The effects of learning rate of individual agents on group performance during the mitigation of civil violence, together with the effects of bias for particular strategies, was researched using a multi-agent 
system approach. Though only minor effects of learning rate were found, a bias for strategy did affect performance drastically. Some improvements to the simulation are necessary to allow the result to 
be translated to the real world. However, ultimately, an agent-based simulation may be of great use for understanding the individual agents' influence on performance, and for the assessment of strategy success. 
