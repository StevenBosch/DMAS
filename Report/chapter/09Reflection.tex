\section{Discussion}
In this paper a multi-agent system approach was taken to identify the effects of individual experience and learning on group performance during a group task, specifically riot control. Several simulations were run using different learning rates - this way simulating the amount of experience of the agents - and different biases towards an action. Results showed few significant differences in performance for different learning rates. Different biases did show some clear differences in performance however, showing an inverse relationship between speed and success given the two actions.

The fact that learning (and in extension different learning rates) had no noticeable effect on success, suggests that the group performance of cops in mitigating civil wars does not depend on personal experience to a large degree. Aside from the possibility that this is indeed the case, there is a number of factors within our simulation that might contribute to this result.
This can be explained by a number of factors: (1) the amount of agents, (2) the uniformity and randomness of the grid, (3) the learning mechanism and (4) the rewards. 

(1) Because of the large number of cops (4000) in the simulation, success of individual agents counts little. Both utilities and successes have a tendency to cancel each other out when looked upon globally. This translates to the real world: within a group of 4000, how much does the individual experience of one agent really matter? 
(2) Because the grid is highly uniform in both its structure (symmetric, identical grids) and population, while initialization happens at random, there are not enough heuristics available to drive the utilities and successes: is there even an optimal solution? (3) Because the cops learn based on the group success per cell, opposing behaviors in one cell can get the same reinforcement, which globally leads to more or less random behavior. (4) Finally, even if the group d\'{i}d learn other behavior as a whole (as was sometimes the case, see figure \ref{fig:utShoot} and \ref{fig:utSave}), the rewards for both saving civilians and incapacitating hostiles being the same caused the global success to be hardly affected by this learning. It is then curious why the cops learned this behavior if not for success. Unfortunately this is a question we have yet to find an answer to.
All in all, the combination of these factors caused the effect of individual learning to be negligible for group performance in our simulation.

The initialized biases, however, did result in big differences in performances. Whereas a bias for saving greatly sped up the mitigation at the cost of success, incapacitating hostiles greatly increased success at the cost of speed. This can be explained by the fact that the stop criterion in the simulation was set at there being no more civilians left in the simulation. If cops were heavily biased towards saving them, civilians would be saved more quickly, but also be killed more quickly, because few hostiles would get incapacitated, giving them the chance to shoot at civilians. This would result in a quick mitigation, but a relatively low success. On the other hand if cops were heavily biased towards incapacitating, hostiles would be removed from the simulation quickly, resulting in less civilians and cops being killed. This would result in a high success rate but an overall low speed, since it takes the cops longer to save all the civilians.

These results indicate that, instead of the composition of the team with regard to experience, the focus should lie on strategy selection, based on the goal at hand. So following these results it would be more important for a police force to determine a strategy for their cops beforehand, than to look very carefully at the composition of the team in terms of experience. The results do need refinement though, because the simulation was an extremely simplified approximation of reality. With hostiles only able to shoot, and the inability for hostiles and civilians to walk through the city, the simulation can clearly be considered a low-fidelity simulation. Moreover, the cops only have one decision to make and only walk when all hostiles are gone in his area.

Considering these downfalls and the potential of such simulations, we recommend a number of improvements and extensions for future research. Important improvements would consist of making the agents more intelligent and the matrix more city like to provide more accurate results. Furthermore this research only focussed on the comparison between saving and incapacitating. By implementing more realistic behavior, it would be possible to 
compare the results of different strategies or combinations thereof. Moreover, new strategies may be designed and tested by use of a multi-agent simulation. Showing what strategies show the most 
potential and how individual cops affect these strategies could ultimately result in better mitigation of civil wars and better training programs. 

\section{Conclusion}
The effects of learning rate of individual agents on group performance during the mitigation of civil violence, together with the effects of bias for particular strategies, was researched using a multi-agent 
system approach. Though only minor effects of learning rate were found, a bias for strategy did affect performance drastically. Some improvements to the simulation are necessary to allow the result to 
be translated to the real world. However, ultimately, an agent-based simulation may be of great use for understanding the individual agents' influence on performance, and for the assessment of strategy success. 
