\section{Discussion}
In this paper a multi-agent system approach was taken to identify the effects of experience on learning during a group task, that is, riot control. Several simulations were run using different learning rates - this way simulating the amount of experience of the agents - and different biases towards an action. Results showed some significant differences in performance, though the effects were marginal. Different biases did show some clear differences in performance, showing an inverse relationship between speed and success given the two actions. Moreover, a preference for actions do indeed affect performance in both speed and success. 

The fact that the differences in performance between different learning rates were so small, suggests that the group performance of cops during mitigating civil wars does not depend on personal experience in a large degree. This can be explained by a number of factors: (1) the amount of agents, (2) the uniformity of the map and (3) randomness of agents' perception. First, averaged over the 4000 cops, success of one's preference will be canceled out by the similar success of opposing behavior. Because the cops learn based on the group success per cell, oppossing behavior in one cell can get the same reinforcement, which ultimately leads to random global behavior. Second, because the map is always completely uniform in both its structure (symmetric, identical grids) and population, agents will often. Finally 

The combination of these factors caused the effect of individual learning to be negligible.

The initialized biases, however, did result in big changes in performances. Whereas a bias for saving greatly sped up the mitigation at the cost of success, incapacitating hostiles greatly increased success at the cost of speed. This can be explained by the fact that the stop criterion in the simulation was set at no more civilians left in the simulation. If cops were heavily biased towards saving them, civilians would both be saved more quickly and be killed more quickly, because few hostiles would get incapacitated. This would result in a quick mitigation, but a relatively low success. On the other hand if cops were heavily biased towards incapacitating, hostiles would be removed from the simulation quickly, resulting in less civilians and cops being killed. This would result in a high success rate but an overall low speed, since it takes the cops longer to save all the civilians.

These results indicate that, instead of the composition of the team with regard to experience, the focus should lie on strategy selection, based on the goal at hand.

Note that the simulation was an extremely simplified approximation of reality. With hostiles only able to hurt, and the inability for hostiles and civilians to walk through the city, the simulation can clearly be considered a low-fidelity simulation. Moreover, the cops only have one decision to make and will only walk when all hostiles are gone in his area. Making the agents, that is, the cops, hostiles and civilians, more intelligent, and the matrix more city like, results can be more meaningful in the real world. 

This simulation does provide potential for future researches. In this paper, the comparison was made between saving and incapacitating. By implementing more realistic behavior, it is possible to 
compare the results of different strategies or combinations thereof. Moreover, new strategies may be designed and tested by use of a multi-agent simulation. Showing what strategies show the most 
potential and how individual cops affect these strategies could ultimately result in better mitigation of civil wars and better training programs. 

\section{Conclusion}
The effects of learning rate of individual agents on group performance during the mitigation of civil violence, together with the effects of bias for particular strategies, was researched using a multi-agent 
system approach. Though only minor effects of learning rate were found, a bias for strategy did affect performance drastically. Some improvements to the simulation are necessary to allow the result to 
be translated to the real world. However, ultimately, an agent-based simulation may be of great use for understanding the individual agents' influence on performance, and for the assessment of strategy success. 
