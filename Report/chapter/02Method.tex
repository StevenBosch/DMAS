\chapter{Method}
The simulation consists three types of agents: cops, hostiles and civilians. The goal of the cops is to keep (civil and cop) casualties as low as possible. The hostiles on the other hand have only one goal. Their only goal is to kill as many civilians as possible, to cause mayhem and despair. Civilians have no particular function, but are subject to the actions taken by cops and hostiles. The simulation takes place in a NxN 2D matrix. In every box of the matrix 20 agents can reside. The agents that reside in this box can only see the 19 other agents in that box. This allows for easier computation of interactions with visible agents. 

\paragraph{Goals and Actions}
The cops goal is to keep casualties as low as possible. To achieve this, the cop must choose one of two actions: shooting a hostile, with a risk of killing a civilian, or saving a civilian, with the risk of being killed by a hostile. Hostiles also have two options. They can either choose to shoot at the civilians with a chance of hitting an agent, or they can shoot at the cops for self defence, with a chance of hitting a civilian. All the cops and hostiles, during every epoch, can perform one of these actions. Based on the outcome of the agent's individual action, a reward is given to this individual agent. This reward can be either positive or negative. If a cop kills a civilian, for instance, a negative reward will be given, whilst when he has killed a hostile, a positive reward will be given. 

\begin{table}[!ht]
\begin{center}
\begin{tabular}{l l | l l}
\hline
 & & Action 1 & Action 2 \\
\hline
\multirow{2}{*}{Size Team 1 > Team 2} & Many civilians & Ux & Uy \\
 & Few civilians & Ur & Ut \\
\hline
\multirow{2}{*}{Size Team 1 < Team 2} & Many civilians & Ua & Ub\\
 & Few civilians & Uc & Ud\\
\hline
\end{tabular}
\caption{The different scenarios an agent can encounter and fictional success values for each action.}
\label{tab:scenarios}
\end{center}
\end{table}

The reward is used to update the utility of the particular actions in particular circumstances. These circumstances are shown in Table \ref{tab:scenarios}. The agents must thus decide, based on the amount of team members, opponents and civilians what action would be most successful. In the beginning of the simulation, all the utilities are set to equal values, such that no biases may exist. Reward of an action can be calculated as following for cops: 
$$ R = (Kills + Saves - Deaths)/(Kills + Saves + Deaths)$$
in which $Kills$ are the amount of killed hostiles, $Saves$ the amount of saved civilians, and $Deaths$ the amount of killed cops. Based on these calculations the reward will lie between -1 (only deaths) and 1 (only kills and saves). For hostiles the reward function is as follows:
$$ R = (Kills - Deaths)/(Kills + Deaths)$$
, in which $Kills$ is the amount of killed civilians and cops, and $Deaths$ the amount of killed fellow hostiles. 

As the Table \ref{tab:scenarios} also shows, there is no situation in which the teams are of equal size. It is assumed that the agents have no perfect knowledge of the area, and, therefore, must decide which team has the overhand. This decision is made according to the following stochastic function:
$$ function that determines which team has the overhand. $$

After each epoch, the agents can decide to move to a neighboring box in the matrix. If the agent decides to move it will go to the place, in which the action with the highest utility will be applied. In other words, the agents will move to where he thrives best. 
\paragraph{Learning}



Q-learning implements a decrese in learning rate by adjusting the learning rate variable $\lambda$ over time (REF). In temporal difference learning this is achieved by storing memories (of actions and results) in time (REF). Over time these memories will decay (less often experienced ones quicker than more experienced ones) allowing a flexible learning of the best actions/strategies. For this simulation, the learning rate variable will be implemented. 



\paragraph{Scenarios}
Overtal/Ondertal, veel civilians/weinig civilians. 
