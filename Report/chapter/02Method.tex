\section{Method}
\subsection{Simulation model}
Three types of agents will be included in this simulation: cops, hostiles and civilians. The goal of the cops is to keep (civil and cop) casualties as low as possible. The hostiles on the other hand have only one goal: to kill as many civilians as possible to cause mayhem and despair. Civilians have no particular function, but are subject to the actions taken by cops and hostiles. The simulation takes place in a 20x20 2D matrix. In every box reside a mean of 20 civilians, 10 hostiles and 9 cops. The agents that reside in this box can only see the other agents in that box. This allows for easier computation of interactions with visible agents. In every simulation, the agents will be randomly spawned in the matrix.  

\subsubsection{Goals and Actions}
The cops' goal is to keep casualties as low as possible. To achieve this, every cop must choose one of two actions. The cop can shoot a hostile, which eliminates a treath of future killings, but at the same time there is the risk of killing a civilian. The second action is to to save a civilian, who is then safe from being killed, but the cop has a higher risk of being killed by a hostile. Hostiles only have one action: to kill. Depending on the amount of civilians and cops, one group has a larger possibility to get killed by the hostiles. When, for example, only cops and hostiles reside in a box, the possibility that cops are targeted is 100 percent. It depends on the amount of hostiles how many civilians and cops may die.

\begin{table}[!ht]
\begin{center}
\begin{tabular}{l l | l l}
\hline
 & & Shoot & Save \\
\hline
\multirow{2}{*}{\# Cops > \# Hostiles} & Many civilians & Ux & Uy \\
 & Few civilians & Ur & Ut \\
\hline
\multirow{2}{*}{\# Cops < \# Hostiles} & Many civilians & Ua & Ub\\
 & Few civilians & Uc & Ud\\
\hline
\end{tabular}
\caption{The different scenarios an agent can encounter and fictional success values for each action.}
\label{tab:scenarios}
\end{center}
\end{table}
All the cops, during every epoch, can either save or shoot once. Based on the overall result in that block, a reward is given to the individual agent. This reward can be either positive or negative. If many civilians die, for instance, a negative reward will be given, whilst when many hostiles were killed, a positive reward will be given. The decision that rewards are given based on group success, as opposed to individual success, is based on the idea that in a crowded situation it is not always clear what the results of individual actions are, but that a general idea of group performance can be perceived. 

The reward is used to update the utility of the particular actions in a specific circumstance. These circumstances are shown in \autoref{tab:scenarios}. The cops must thus decide, based on the amount of team members, opponents and civilians what action would be most successful, that is, which action has the highest utility. Because the agents are not perfect, their judgement of the situation and the decision for an action may be erroneous. Mistakes can thus be made, which increases the realism of the simulation. 

In the beginning of the simulation, all the utilities are set to equal values, such that no biases may exist. Reward of an action can be calculated as following: 
$$ R(s,a) = \frac{Kills(s) + Saves(s') - Losses(s')}{Kills(s') + Saves(s') + Losses(s')}$$
in which $R$ is the reward of action $a$ in situation $s$, $Kills$ are the amount of killed hostiles in the new situation $s'$, $Saves$ the amount of saved civilians, and $Losses$ the amount of killed cops and civilians. Based on these calculations the reward will lie between -1 (only deaths) and 1 (only kills and saves). Because hostiles only have one action, no learning is necessary. 

As \autoref{tab:scenarios} also shows, there is no situation in which the teams are of equal size. The cops must decide whether they have the overhand or not, which is done according to the following function:
$$ \Omega = \frac{n_{cops}}{n_{cops} + n_{hostiles}}*\sigma $$
in which $\Omega$ is a value between 0 and 1, $n$ is the amount of agents of a group in that box and $\sigma$ is a random value between 0.5 and 1.5 to make the decision stochastic. It is assumed namely that the cops have no perfect knowledge of their environmnet. If $\Omega$ is higher than $0.5$, the cops assume they have the overhand. The same function is used to assess the amount of civilians; The number of cops is then replaced by the amount of civilians. 

When there is no threat in the area of an cop anymore, that is, if there are no more hostiles left, it can move to one of (at most) four neighboring boxes in the matrix. The cop will move to the cell where he is needed most. In other words, the cops will move to where there is the biggest discrepency between the amount of agents and the sum of hostiles and civilians. This way, the cops can always search for a place where he can be of use. 

\subsubsection{Learning}
Many reinforcement learning algorithms, such as Q-learning, have implemented the fact that newer experiences have a reduced influence  \citep*{watkins1992q}. This is implemented with a learning rate variable $\lambda$. High learning rates allow agents to learn more quickly compared to a low learning rate, but this also allows the agents to switch to suboptimal strategies more often. This may result in a lower performance compared to the low learning rate. Gradually changing the learning rate over time allows the function to converge to the true Q-value, that is the utility value. In the current simulation, a similar approach will be taken. 

The utility will be updated according to the following function:
$$ U(s,a)_{new}(t) = U(s,a)_{old} + \frac{\lambda * R(s,a)}{1+\lambda} $$
in which $U_x(t)$ is the new utility (ranging from 0 to 1) of an action $a$ in situation $s$, $t$ is the moment in time, $\lambda$ is the learning rate factor ranging from 0 to 1, and $R(t)$ is the reward, as calculated before. After each epoch, in which one action could be taken by each agent, the utility functions will be updated. Following the utilities will be normalized, such that the utility of the two actions in a particular situation will sum up to one. In a way, the success of one action will also mean the discounting of another, therefore producing a stronger preference for what is successfull. According to the formula, henever there is a reward of zero, no changes in utility will occur

\subsection{Experimental Design}
To see the influence of learning rates on behavioral learning, 1000 runs, that is, 1000 civil wars were run. In the first four simulations the agents receive a learning rate of $0.8$, $0.5$, $0.2$ and $0.0$ respectively, which will be fixed over the 100 runs. In the final simulation, the agents will start with a high learning rate value of $0.8$, which will be decreased by a factor of $0.05$ every run. In other words, people gain experience on the job and will become less adaptive with every civil war. 

During the simulations, we will record the global success value of each run and the amount of epochs of these runs. Moreover, the global mean utility table of every run will be saved. The simulation of one riot is finished when there are no civilians left in the area. 

One simulation is considered to be more successful if the mean successvalue, which is the same as the reward function, was significantly higher compared to the other simulations. If no difference in success is found, we will look at the amount of epochs it takes to finish a simulation, that is, the speed of solving the problem. After every run, the utility tables will be remembered and used to initialize agents in the next run. This allows the agents to learn over the different civil wars. 

\subsubsection{No bias}
In the first simulations, the focus lied on which behaviours would be most useful. Therefore, the all the agents were initialized with the default utility values, which is 0.5 for both actions in all situations. It is expected that the tendency to save will be higher when there are many civilians and that the tendency to shoot is higher when many hostiles are present. Learning this behavior should over time increase performance, which should show either in success or in speed. 

\subsubsection{Initial biases}
Another point of interest is how behavior evolves over time, when bias does exist. Can old dog learn new tricks, and how quickly could that happen. To investigate this, a preference of either shooting or saving was given by setting the utility to 0.9 for one action, and 0.1 for the other. When a bias for shooting exists, there will be no more hostiles to shoot after a while. It is expected that after some iterations, the cops learn that saving is more usefull. Similarly, when cops only save, it is expected that shooting will receive a higher utility over time. For the cops having a lower learning rate this will most likely happen more gradually compared to the cops having a higher learning rate.

It is expected that the cops with the lower learnig rates will show higher performance compared to the cops with a medium and higher learning rate. Hypotheses... TODO